{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tired-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "turned-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-million",
   "metadata": {},
   "source": [
    "Download eng embedding from and [look for GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "For french embedding:\n",
    "in the terminal, type (in one line) curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pointed-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to download and process the full dataset on your local computer\n",
    "\n",
    "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "representative-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get the positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tracked-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4440918 , -0.53891373, -0.87011719, -0.12850189,  0.14611816])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "# preprocess tweet\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "stopwords_eng = stopwords.words(\"english\")\n",
    "punctuations_eng = string.punctuation\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r\"\\$\\w*\", \"\", tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r\"^RT[\\s]+\", \"\", tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r\"https?:\\/\\/[.a-zA-Z\\/-]*[\\r\\n]*\", \"\", tweet)\n",
    "    \n",
    "    # remove the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweet_tokens_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_eng and word not in punctuations_eng):\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            \n",
    "            tweet_tokens_clean.append(stemmed_word)\n",
    "    \n",
    "    return tweet_tokens_clean\n",
    "\n",
    "def get_doc_embedding(tweet, en_embeddings):\n",
    "    doc_embedding = np.zeros(300)\n",
    "    \n",
    "    preprocessed_tweet = preprocess_tweet(tweet)\n",
    "    \n",
    "    for word in preprocessed_tweet:\n",
    "        if word in en_embeddings:\n",
    "            doc_embedding += en_embeddings[word]\n",
    "        \n",
    "    return doc_embedding\n",
    "    \n",
    "\n",
    "# testing your function\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "tweet_embedding = get_doc_embedding(custom_tweet, en_embeddings)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "powered-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_to_embedding_dict_and_embedding_matrix(all_tweets, en_embeddings):\n",
    "    embedding_matrix = []\n",
    "    idx_to_embedding = {}\n",
    "    for idx, tweet in enumerate(all_tweets):\n",
    "        doc_embedding = get_doc_embedding(tweet, en_embeddings)\n",
    "        embedding_matrix.append(doc_embedding)\n",
    "        idx_to_embedding[idx] = doc_embedding\n",
    "        \n",
    "    embedding_matrix = np.vstack(embedding_matrix)\n",
    "    \n",
    "    return embedding_matrix, idx_to_embedding\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "higher-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, idx_to_embedding = get_idx_to_embedding_dict_and_embedding_matrix(all_tweets, en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automated-hindu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dictionary {len(idx_to_embedding)}\")\n",
    "print(f\"shape of document_vecs {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "digital-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9838\n",
      "being sad for no reason sucks because u dunno how to stop being sad so u just gotta chill in ur room and listen to music &amp; b alone :(\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'i am sad'\n",
    "tweet_embedding = get_doc_embedding(my_tweet, en_embeddings)\n",
    "\n",
    "idx = np.argmax(cosine_similarity(embedding_matrix, tweet_embedding))\n",
    "print(idx)\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "disturbed-lighting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_VECS, EMBEDDING_DIM = embedding_matrix.shape\n",
    "NUM_VECS, EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "resistant-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_value(vec, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dot_product = np.dot(vec, planes)\n",
    "    \n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "    \n",
    "    \n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    h = sign_of_dot_product>=0\n",
    "    \n",
    "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    hash_value = 0\n",
    "    \n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        hash_value += np.power(2,i)*h[i]\n",
    "    \n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "\n",
    "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25\n",
    "\n",
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(EMBEDDING_DIM, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]\n",
    "vec = np.random.rand(1, 300)\n",
    "hash_value = get_hash_value(vec, planes_l[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "mexican-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vectors, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = 2**num_of_planes\n",
    "    \n",
    "    hash_table = {idx: [] for idx in range(num_buckets)}\n",
    "    id_table = {idx: [] for idx in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for idx, vec in enumerate(vectors):\n",
    "        # calculate the hash value for the vector\n",
    "        h = get_hash_value(vec, planes)\n",
    "\n",
    "        hash_table[h].append(vec)\n",
    "        id_table[h].append(idx)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "specific-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grnerate table\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(embedding_matrix, planes)\n",
    "    \n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "intense-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[177 192 194]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6595, 8398, 8588]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def k_nearest_neighbours(v, candidates, k=1):\n",
    "    \n",
    "    neighbours = []\n",
    "    for candidate in candidates:\n",
    "        sim = cosine_similarity(v, candidate)\n",
    "        neighbours.append(sim)\n",
    "        \n",
    "    sorted_idxs = np.argsort(neighbours)\n",
    "    \n",
    "    return sorted_idxs[-k:]\n",
    "\n",
    "def approx_knn(vec, planes_l, num_universes, k=1):\n",
    "    candidate_vectors = []\n",
    "    candidate_ids = []\n",
    "    candidate_id_set = set()\n",
    "    \n",
    "    for u_id in range(num_universes):\n",
    "        planes = planes_l[u_id]\n",
    "        \n",
    "        hash_table = hash_tables[universe_id]\n",
    "        id_table = id_tables[universe_id]\n",
    "        \n",
    "        hash_value = get_hash_value(vec, planes)\n",
    "        \n",
    "        # doc vectors for this universe id\n",
    "        doc_vectors = hash_table[hash_value]\n",
    "        doc_ids = id_table[hash_value]\n",
    "        \n",
    "        for idx, doc_id in enumerate(doc_ids):\n",
    "            \n",
    "            if doc_id not in candidate_id_set:\n",
    "#                 print(idx, doc_id)\n",
    "                \n",
    "                candidate_vectors.append(doc_vectors[idx])\n",
    "                candidate_ids.append(doc_id)\n",
    "                \n",
    "                candidate_id_set.add(doc_id)\n",
    "                \n",
    "    candidate_vectors = np.array(candidate_vectors)\n",
    "    nearest_neighbor_ids = k_nearest_neighbours(vec, candidate_vectors, k=k)\n",
    "    print(nearest_neighbor_ids)\n",
    "\n",
    "    nearest_neighbor_ids = [candidate_ids[idx]\n",
    "                        for idx in nearest_neighbor_ids]\n",
    "\n",
    "    return nearest_neighbor_ids\n",
    "\n",
    "        \n",
    "\n",
    "vec = get_doc_embedding(\"I am sad\", en_embeddings)\n",
    "approx_knn(vec, planes_l, 25, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "animal-martial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nobodies up with me now, I'm sad :(\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[8588]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-jungle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
